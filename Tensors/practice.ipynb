{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f2f153",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Mastery Practice\n",
    "\n",
    "This notebook contains 50+ progressive questions to master PyTorch tensors. Work through them in order, as they build on each other.\n",
    "\n",
    "## How to Use This Practice Guide\n",
    "\n",
    "1. Read each question carefully\n",
    "2. Implement the solution in the code cell below each question\n",
    "3. Test your implementation with different inputs\n",
    "4. Check edge cases and error handling\n",
    "5. Compare your approach with PyTorch's built-in functions when applicable\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing these exercises, you will master:\n",
    "\n",
    "- Tensor creation and initialization\n",
    "- Shape manipulation and broadcasting\n",
    "- Mathematical operations and reductions\n",
    "- Device management (CPU/GPU)\n",
    "- Memory management and efficiency\n",
    "- Advanced indexing and slicing\n",
    "- Real-world deep learning tensor patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ded9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09194a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SECTION 1: BASIC TENSOR CREATION (1-15)\n",
    "# ========================================\n",
    "\n",
    "# 1. Create a 2x3 tensor filled with zeros using torch.zeros()\n",
    "# ✅ CORRECT: Good implementation with explicit dtype and requires_grad\n",
    "# Note: requires_grad=False is default, so it's optional\n",
    "x = torch.zeros(2, 3, dtype=torch.int16, requires_grad=False)\n",
    "print(f\"Q1 - Shape: {x.shape}, Dtype: {x.dtype}\")\n",
    "print(x)\n",
    "\n",
    "# 2. Create a 4x4 identity matrix using torch.eye()\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "identity = torch.eye(4)\n",
    "print(f\"\\nQ2 - Identity matrix shape: {identity.shape}\")\n",
    "print(identity)\n",
    "\n",
    "# 3. Create a tensor from the Python list [[1, 2, 3], [4, 5, 6]] and print its shape\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"\\nQ3 - Shape: {x.shape}\")\n",
    "print(x)\n",
    "\n",
    "# 4. Create a random tensor of shape (3, 5) with values between 0 and 1\n",
    "# ✅ CORRECT: torch.rand() creates values between 0 and 1\n",
    "random_tensor = torch.rand(3, 5)\n",
    "print(\n",
    "    f\"\\nQ4 - Shape: {random_tensor.shape}, Min: {random_tensor.min():.3f}, Max: {random_tensor.max():.3f}\"\n",
    ")\n",
    "\n",
    "# 5. Create a tensor of shape (2, 4) filled with the value 7.5\n",
    "# ✅ CORRECT: Good approach using torch.ones() * 7.5\n",
    "# Alternative: torch.full((2, 4), 7.5)\n",
    "filled_tensor = torch.ones(2, 4) * 7.5\n",
    "print(f\"\\nQ5 - Shape: {filled_tensor.shape}, Unique values: {filled_tensor.unique()}\")\n",
    "# Alternative approach:\n",
    "filled_alt = torch.full((2, 4), 7.5)\n",
    "print(f\"Alternative approach: {filled_alt.unique()}\")\n",
    "\n",
    "# 6. Convert the NumPy array np.array([1.1, 2.2, 3.3]) to a PyTorch tensor\n",
    "# ❌ INCOMPLETE: You didn't complete this question separately\n",
    "np_array = np.array([1.1, 2.2, 3.3])\n",
    "torch_from_numpy = torch.from_numpy(np_array)\n",
    "print(f\"\\nQ6 - Original NumPy: {np_array}\")\n",
    "print(f\"Q6 - Converted tensor: {torch_from_numpy}\")\n",
    "print(\n",
    "    f\"Q6 - Shares memory: {torch_from_numpy.data_ptr() == np_array.__array_interface__['data'][0]}\"\n",
    ")\n",
    "\n",
    "# 7. Create a tensor like an existing tensor but filled with ones (use torch.ones_like)\n",
    "# ✅ CORRECT: Good implementation, but you combined it with Q6\n",
    "# Let's separate them for clarity\n",
    "reference_tensor = torch.from_numpy(np_array)\n",
    "ones_like_tensor = torch.ones_like(reference_tensor)\n",
    "print(\n",
    "    f\"\\nQ7 - Reference shape: {reference_tensor.shape}, dtype: {reference_tensor.dtype}\"\n",
    ")\n",
    "print(f\"Q7 - Ones like: {ones_like_tensor}\")\n",
    "\n",
    "# 8. Create a linearly spaced tensor from 0 to 10 with 50 points (use torch.linspace)\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "linspace_tensor = torch.linspace(0, 10, 50)\n",
    "print(f\"\\nQ8 - Shape: {linspace_tensor.shape}\")\n",
    "print(f\"Q8 - First 5 values: {linspace_tensor[:5]}\")\n",
    "print(f\"Q8 - Last 5 values: {linspace_tensor[-5:]}\")\n",
    "\n",
    "# 9. Create a tensor with random integers between 1 and 100, shape (3, 3)\n",
    "# ⚠️ ISSUE: torch.rand(3,3)*100 gives floats between 0-100, not integers 1-100\n",
    "# Your approach:\n",
    "float_random = torch.rand(3, 3) * 100\n",
    "print(f\"\\nQ9 - Your approach (floats 0-100): {float_random.dtype}\")\n",
    "print(f\"Sample values: {float_random[0, :3]}\")\n",
    "\n",
    "# Correct approaches for integers 1-100:\n",
    "int_random_v1 = torch.randint(1, 101, (3, 3))  # Method 1: direct randint\n",
    "int_random_v2 = (torch.rand(3, 3) * 99 + 1).int()  # Method 2: scale and convert\n",
    "print(f\"\\nQ9 - Correct approach 1: {int_random_v1.dtype}\")\n",
    "print(int_random_v1)\n",
    "print(f\"Q9 - Correct approach 2: {int_random_v2.dtype}\")\n",
    "print(int_random_v2)\n",
    "\n",
    "# 10. Create a tensor using torch.arange from 0 to 20 with step size 2\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "arange_tensor = torch.arange(0, 20, 2)\n",
    "print(f\"\\nQ10 - Values: {arange_tensor}\")\n",
    "print(f\"Q10 - Shape: {arange_tensor.shape}\")\n",
    "\n",
    "# 11. Create a 3D tensor of shape (2, 3, 4) filled with random normal distribution\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "normal_tensor = torch.randn(2, 3, 4)\n",
    "print(f\"\\nQ11 - Shape: {normal_tensor.shape}\")\n",
    "print(f\"Q11 - Mean: {normal_tensor.mean():.3f}, Std: {normal_tensor.std():.3f}\")\n",
    "\n",
    "# 12. Create a tensor and explicitly set its dtype to torch.float64\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "float64_tensor = torch.randn(3, 3, dtype=torch.float64)\n",
    "print(f\"\\nQ12 - Shape: {float64_tensor.shape}, Dtype: {float64_tensor.dtype}\")\n",
    "\n",
    "# 13. Create a boolean tensor of shape (3, 3) with random True/False values\n",
    "# ❌ INCORRECT: torch.rand(3,3, dtype=torch.bool) doesn't work as expected\n",
    "# Your approach creates all True values because any non-zero float becomes True\n",
    "try:\n",
    "    bool_wrong = torch.rand(3, 3, dtype=torch.bool)\n",
    "    print(f\"\\nQ13 - Your approach result: {bool_wrong}\")\n",
    "except:\n",
    "    print(\"\\nQ13 - Your approach may not work as expected\")\n",
    "\n",
    "# Correct approaches:\n",
    "bool_correct_v1 = torch.randint(0, 2, (3, 3), dtype=torch.bool)  # Method 1\n",
    "bool_correct_v2 = torch.rand(3, 3) > 0.5  # Method 2\n",
    "print(f\"Q13 - Correct approach 1: {bool_correct_v1.dtype}\")\n",
    "print(bool_correct_v1)\n",
    "print(f\"Q13 - Correct approach 2: {bool_correct_v2.dtype}\")\n",
    "print(bool_correct_v2)\n",
    "\n",
    "# 14. Create a tensor from a nested list with mixed data types and observe the result\n",
    "# ❌ INCOMPLETE: You didn't implement this\n",
    "mixed_list = [[1, 2.5, 3], [4.0, 5, 6.7]]  # Mix of int and float\n",
    "mixed_tensor = torch.tensor(mixed_list)\n",
    "print(f\"\\nQ14 - Mixed data types list: {mixed_list}\")\n",
    "print(f\"Q14 - Resulting tensor: {mixed_tensor}\")\n",
    "print(f\"Q14 - Tensor dtype: {mixed_tensor.dtype} (PyTorch promotes to common type)\")\n",
    "\n",
    "# Example with incompatible types:\n",
    "try:\n",
    "    incompatible = [[1, 2], [\"a\", \"b\"]]  # int and string\n",
    "    bad_tensor = torch.tensor(incompatible)\n",
    "except Exception as e:\n",
    "    print(f\"Q14 - Error with incompatible types: {e}\")\n",
    "\n",
    "# 15. Create a tensor and clone it (ensuring no memory sharing)\n",
    "# ✅ CORRECT: Perfect implementation\n",
    "x = torch.rand(2, 3)\n",
    "y = x.clone()\n",
    "print(f\"\\nQ15 - Original tensor: {x}\")\n",
    "print(f\"Q15 - Cloned tensor: {y}\")\n",
    "print(f\"Q15 - Same data: {torch.equal(x, y)}\")\n",
    "print(f\"Q15 - Share memory: {x.data_ptr() == y.data_ptr()}\")\n",
    "\n",
    "# Verify no memory sharing:\n",
    "x[0, 0] = 999\n",
    "print(f\"Q15 - After modifying original: x[0,0]={x[0,0]}, y[0,0]={y[0,0]}\")\n",
    "print(\n",
    "    f\"Q15 - Memory sharing test: {'❌ SHARED' if x[0,0] == y[0,0] else '✅ INDEPENDENT'}\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SECTION 1 SUMMARY:\")\n",
    "print(\"✅ Correct: Questions 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 15\")\n",
    "print(\"⚠️  Issues: Question 9 (should use randint), Question 13 (bool generation)\")\n",
    "print(\"❌ Incomplete: Questions 6 (merged with 7), 14 (not implemented)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96699bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SECTION 2: SHAPE MANIPULATION (16-30)\n",
    "# ==========================================\n",
    "\n",
    "# 16. Reshape a tensor of shape (12,) to shape (3, 4) using .view()\n",
    "x = torch.rand(\n",
    "    12,\n",
    ")\n",
    "x = x.view(3, 4)\n",
    "\n",
    "# 17. Reshape the same tensor to (2, 6) using .reshape()\n",
    "x = x.reshape(2, 6)\n",
    "\n",
    "# 18. Explain the difference between .view() and .reshape() with an example\n",
    "# --- Ans: .view works faster it is a way to represent the same tensor without changing its contiguos memory, but reshape will attempt to reconstruct the tensor\n",
    "\n",
    "# 19. Use .squeeze() to remove all dimensions of size 1 from a tensor\n",
    "x = torch.rand(1, 2, 4, 5)\n",
    "x.squeeze()\n",
    "\n",
    "# 20. Use .unsqueeze() to add a new dimension at position 1\n",
    "x.unsqueeze(dim=1)\n",
    "\n",
    "# 21. Transpose a 2D tensor (swap rows and columns)\n",
    "x = torch.rand(4, 5)\n",
    "x.transpose(0, 1)\n",
    "\n",
    "# 22. Use .permute() to rearrange dimensions of a 4D tensor (B,C,H,W) to (B,H,W,C)\n",
    "x = torch.rand(1, 2, 4, 5)\n",
    "x.permute(0, 2, 3, 1)\n",
    "\n",
    "# 23. Flatten a 3D tensor to 1D using .flatten()\n",
    "x = torch.rand(2, 4, 5)\n",
    "x.flatten()\n",
    "\n",
    "# 24. Flatten only the last two dimensions of a 4D tensor\n",
    "x = torch.rand(2, 4, 5, 3)\n",
    "x.flatten(start_dim=2)\n",
    "\n",
    "# 25. Concatenate two tensors along dimension 0 using torch.cat()\n",
    "x = torch.rand(2, 4)\n",
    "y = torch.rand(2, 4)\n",
    "torch.cat([x, y], dim=0)\n",
    "\n",
    "# 26. Stack two 2D tensors to create a 3D tensor using torch.stack()\n",
    "x = torch.rand(2, 4)\n",
    "y = torch.rand(2, 4)\n",
    "torch.stack([x, y], dim=0)\n",
    "\n",
    "# 27. Split a tensor into 3 equal parts along dimension 1\n",
    "x = torch.rand(5, 6)\n",
    "torch.chunk(x, 3, dim=1)\n",
    "\n",
    "# 28. Use torch.chunk() to divide a tensor into 4 chunks\n",
    "x = torch.rand(5, 8)\n",
    "torch.chunk(x, 4, dim=1)\n",
    "\n",
    "# 29. Repeat a tensor 3 times along dimension 0 using .repeat()\n",
    "x = torch.rand(5, 8)\n",
    "torch.repeat_interleave(x, 4, dim=0)\n",
    "\n",
    "# 30. Expand a tensor from shape (1, 3) to shape (5, 3) without copying data\n",
    "\n",
    "# I dont know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b111446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  6],\n",
       "         [ 3,  9]],\n",
       "\n",
       "        [[ 1,  7],\n",
       "         [ 4, 10]],\n",
       "\n",
       "        [[ 2,  8],\n",
       "         [ 5, 11]]], dtype=torch.int16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SECTION 3: MATHEMATICAL OPERATIONS (31-45)\n",
    "# ==========================================\n",
    "\n",
    "# 31. Perform element-wise multiplication of two tensors\n",
    "x = torch.rand(2, 4)\n",
    "y = torch.rand(2, 4)\n",
    "x * y\n",
    "\n",
    "# 32. Compute matrix multiplication of two 2D tensors using torch.matmul()\n",
    "\n",
    "x = torch.rand(2, 4)\n",
    "y = torch.rand(4, 2)\n",
    "x.matmul(y)\n",
    "\n",
    "# 33. Use torch.bmm() for batch matrix multiplication (3D tensors)\n",
    "x = torch.rand(5, 2, 4)\n",
    "y = torch.rand(5, 4, 2)\n",
    "torch.bmm(x, y)\n",
    "\n",
    "# 34. Calculate the mean of a tensor along dimension 1\n",
    "x.mean(dim=1)\n",
    "\n",
    "# 35. Find the maximum value and its index using torch.max()\n",
    "index, value = torch.max(x, dim=1)\n",
    "\n",
    "# 36. Compute the standard deviation of a tensor along the last dimension\n",
    "x.std(dim=-1)\n",
    "\n",
    "# 37. Apply torch.clamp() to restrict tensor values between -1 and 1\n",
    "torch.clamp(x, -1, 1)\n",
    "\n",
    "# 38. Use torch.where() to replace negative values with zeros\n",
    "torch.where(x < 0, 0, x)\n",
    "\n",
    "# 39. Compute the L2 norm of a tensor\n",
    "torch.norm(x)\n",
    "\n",
    "# 40. Calculate the dot product of two 1D tensors\n",
    "# ❌ FIXED: Use 1D tensors for torch.dot\n",
    "x1 = torch.rand(5)\n",
    "y1 = torch.randn(5)\n",
    "torch.dot(x1, y1)\n",
    "\n",
    "# 41. Implement ReLU activation function using tensor operations\n",
    "torch.where(x > 0, x, 0)\n",
    "\n",
    "# 42. Calculate softmax probabilities for a batch of logits\n",
    "# ❌ FIXED: Use torch.rand for logits and correct dim\n",
    "logits = torch.rand(10, 1, 5)\n",
    "torch.softmax(logits, dim=-1)\n",
    "\n",
    "# 43. Compute the cross-entropy loss between predictions and targets\n",
    "# ❌ FIXED: Use CrossEntropyLoss for classification\n",
    "pred = torch.randn(5, 4)  # logits\n",
    "labels = torch.randint(0, 4, (5,))  # class indices\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss(pred, labels)\n",
    "\n",
    "\n",
    "# 44. Implement a function to normalize a tensor (zero mean, unit variance)\n",
    "# ⚠️ SIMPLIFIED: Use input_tensor.std(dim=0, unbiased=False)\n",
    "def normalize_tensor(input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    mean = input_tensor.mean(dim=0)\n",
    "    std = input_tensor.std(dim=0, unbiased=False)\n",
    "    return (input_tensor - mean) / std\n",
    "\n",
    "\n",
    "# 45. Calculate cosine similarity between two vectors\n",
    "# ❌ FIXED: Use 1D tensors for cosine similarity\n",
    "x2 = torch.rand(10)\n",
    "y2 = torch.rand(10)\n",
    "torch.dot(x2, y2) / (torch.norm(x2) * torch.norm(y2))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 4: INDEXING & SLICING (46-55)\n",
    "# ==========================================\n",
    "\n",
    "# 46. Extract the first row and last column from a 2D tensor\n",
    "# ✅ Correct: x[0, :] gives first row, x[:, -1] gives last column\n",
    "x = torch.rand(5, 5)\n",
    "first_row = x[0, :]\n",
    "last_col = x[:, -1]\n",
    "\n",
    "# 47. Use boolean indexing to select elements greater than 0.5\n",
    "# ✅ Correct: x[x > 0.5] selects elements > 0.5\n",
    "selected = x[x > 0.5]\n",
    "\n",
    "# 48. Extract every other element from a 1D tensor using slicing\n",
    "# ⚠️ Improved: Use slicing for clarity\n",
    "x = torch.rand(10)\n",
    "every_other = x[::2]\n",
    "\n",
    "# 49. Use advanced indexing to gather specific elements from multiple dimensions\n",
    "# ❌ FIXED: Example using torch.gather for advanced indexing\n",
    "x = torch.arange(16).reshape(4, 4)\n",
    "rows = torch.tensor([0, 1, 2])\n",
    "cols = torch.tensor([1, 2, 3])\n",
    "advanced = x[rows, cols]  # gathers (0,1), (1,2), (2,3)\n",
    "\n",
    "# 50. Implement fancy indexing to reorder rows of a matrix\n",
    "# ❌ FIXED: Example of reordering rows\n",
    "x = torch.arange(12).reshape(4, 3)\n",
    "order = torch.tensor([2, 0, 3, 1])\n",
    "reordered = x[order]\n",
    "\n",
    "# 51. Use torch.gather() to select elements along a dimension\n",
    "# ❌ FIXED: Example for dim=1\n",
    "x = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "indices = torch.tensor([[2, 1, 0], [0, 2, 1]])\n",
    "gathered = torch.gather(x, 1, indices)\n",
    "\n",
    "# 52. Mask out certain elements of a tensor and replace with a value\n",
    "# ❌ FIXED: Use torch.where to mask\n",
    "x = torch.arange(6).float()\n",
    "mask = x > 2\n",
    "masked = torch.where(mask, torch.tensor(-1.0), x)\n",
    "\n",
    "# 53. Extract a diagonal from a 2D tensor\n",
    "# ❌ FIXED: Use torch.diag or torch.diagonal\n",
    "x = torch.arange(9).reshape(3, 3)\n",
    "diag = torch.diagonal(x)\n",
    "\n",
    "# 54. Use torch.nonzero() to find indices of non-zero elements\n",
    "# ✅ Correct: torch.nonzero(x)\n",
    "x = torch.tensor([0, 1, 0, 2])\n",
    "nonzero_indices = torch.nonzero(x)\n",
    "\n",
    "# 55. Implement tensor indexing that mimics NumPy's ix_() function\n",
    "# ❌ FIXED: Use torch.meshgrid for ix_ functionality\n",
    "row_idx = torch.tensor([0, 2])\n",
    "col_idx = torch.tensor([1, 3])\n",
    "ix_rows, ix_cols = torch.meshgrid(row_idx, col_idx, indexing=\"ij\")\n",
    "# Use for advanced indexing: x[ix_rows, ix_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50271c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 2050', major=8, minor=6, total_memory=4095MB, multi_processor_count=16, uuid=8b95a0f1-0127-85af-367b-bf481d8c9078, L2_cache_size=1MB)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SECTION 5: DEVICE MANAGEMENT (56-65)\n",
    "# ==========================================\n",
    "\n",
    "# 56. Check if CUDA is available and print GPU information\n",
    "# ✅ Correct: Checks CUDA and prints device info\n",
    "if torch.cuda.is_available():\n",
    "    info = torch.cuda.get_device_properties(0)\n",
    "    print(info)\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n",
    "\n",
    "# 57. Move a tensor to GPU (if available) and back to CPU\n",
    "# ⚠️ Improved: Use .to() and .cpu() for round-trip\n",
    "x = torch.rand(3, 3)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.to(\"cuda\")\n",
    "    x_cpu = x_gpu.to(\"cpu\")\n",
    "\n",
    "# 58. Create a tensor directly on GPU device\n",
    "# ⚠️ Improved: Only create on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    y = torch.rand(3, 3, device=\"cuda\")\n",
    "else:\n",
    "    y = torch.rand(3, 3)\n",
    "\n",
    "# 59. Perform operations between tensors on the same device\n",
    "# ✅ Correct: x and y must be on the same device\n",
    "if x.device == y.device:\n",
    "    result = x + y\n",
    "else:\n",
    "    result = x.to(y.device) + y\n",
    "\n",
    "# 60. Handle device mismatch errors gracefully\n",
    "# ❌ FIXED: Example with try/except\n",
    "try:\n",
    "    z = x + y\n",
    "except RuntimeError as e:\n",
    "    print(f\"Device mismatch error: {e}\")\n",
    "\n",
    "# 61. Compare performance of CPU vs GPU tensor operations\n",
    "# ❌ FIXED: Use torch.cuda.synchronize and time module\n",
    "import time\n",
    "\n",
    "x_cpu = torch.rand(10000, 10000)\n",
    "start = time.time()\n",
    "res_cpu = x_cpu @ x_cpu\n",
    "cpu_time = time.time() - start\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.to(\"cuda\")\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    res_gpu = x_gpu @ x_gpu\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"CPU time: {cpu_time:.4f}s, GPU time: {gpu_time:.4f}s\")\n",
    "else:\n",
    "    print(f\"CPU time: {cpu_time:.4f}s, GPU not available.\")\n",
    "\n",
    "\n",
    "# 62. Implement a function that ensures tensors are on the correct device\n",
    "# ❌ FIXED: Function to move tensor to target device\n",
    "def ensure_device(tensor, device):\n",
    "    return tensor.to(device)\n",
    "\n",
    "\n",
    "# 63. Use torch.cuda.synchronize() to measure accurate GPU timing\n",
    "# ❌ FIXED: Example usage for timing\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = x_gpu @ x_gpu\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Elapsed (GPU, synchronized): {elapsed:.4f}s\")\n",
    "\n",
    "# 64. Monitor GPU memory usage during tensor operations\n",
    "# ❌ FIXED: Use torch.cuda.memory_allocated()\n",
    "if torch.cuda.is_available():\n",
    "    mem = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU memory allocated: {mem} bytes\")\n",
    "\n",
    "# 65. Implement device-agnostic code that works on both CPU and GPU\n",
    "# ⚠️ Improved: Use device variable and move tensors\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.rand(3, 3, device=device)\n",
    "y = torch.rand(3, 3, device=device)\n",
    "result = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SECTION 6: MEMORY & PERFORMANCE (66-75)\n",
    "# ==========================================\n",
    "\n",
    "# 66. Demonstrate the difference between .clone() and direct assignment\n",
    "\n",
    "# 67. Use in-place operations (.add_(), .mul_()) and understand their implications\n",
    "\n",
    "# 68. Implement a function that checks if two tensors share memory\n",
    "\n",
    "# 69. Create a tensor that shares storage with another tensor using .view()\n",
    "\n",
    "# 70. Use torch.no_grad() context manager and understand when to use it\n",
    "\n",
    "# 71. Implement memory-efficient tensor concatenation for large tensors\n",
    "\n",
    "# 72. Compare memory usage of different tensor creation methods\n",
    "\n",
    "# 73. Use torch.utils.benchmark to measure operation performance\n",
    "\n",
    "# 74. Implement a memory profiler for tensor operations\n",
    "\n",
    "# 75. Demonstrate broadcasting rules with tensors of different shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# SECTION 7: REAL-WORLD APPLICATIONS (76-85)\n",
    "# ===========================================\n",
    "\n",
    "# 76. Implement one-hot encoding for class labels\n",
    "# Hint: Use torch.nn.functional.one_hot or scatter_ for one-hot encoding. Input: class indices, Output: one-hot tensor.\n",
    "# Steps:\n",
    "# 1. Create a tensor of class indices (e.g., torch.tensor([0, 2, 1])).\n",
    "# 2. Use torch.nn.functional.one_hot(indices, num_classes) to get one-hot encoding.\n",
    "class_indices = torch.randint(low=0, high=11, size=(20,))\n",
    "torch.nn.functional.one_hot(class_indices, 10)\n",
    "\n",
    "# 77. Create a function to pad sequences to the same length\n",
    "# Hint: Use torch.nn.utils.rnn.pad_sequence or manual padding with torch.zeros.\n",
    "# Steps:\n",
    "# 1. Given a list of 1D tensors of varying lengths.\n",
    "# 2. Find the max length.\n",
    "# 3. Pad each tensor with zeros (or a specified value) to max length.\n",
    "# 4. Stack into a 2D tensor.\n",
    "\n",
    "# 78. Implement image normalization (ImageNet statistics)\n",
    "# Hint: Subtract mean and divide by std for each channel.\n",
    "# Steps:\n",
    "# 1. Given an image tensor of shape (C, H, W) or (N, C, H, W).\n",
    "# 2. Use mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225].\n",
    "# 3. Normalize: (img - mean) / std (broadcasting over channels).\n",
    "\n",
    "# 79. Create a sliding window operation for time series data\n",
    "# Hint: Use unfold or manual slicing to create overlapping windows.\n",
    "# Steps:\n",
    "# 1. Given a 1D tensor and window size (and optional stride).\n",
    "# 2. Use x.unfold(0, window_size, stride) to get windows.\n",
    "\n",
    "# 80. Implement k-means clustering initialization with tensors\n",
    "# Hint: Randomly select k points as initial centroids from data.\n",
    "# Steps:\n",
    "# 1. Given a data tensor of shape (N, D) and k.\n",
    "# 2. Randomly sample k indices without replacement.\n",
    "# 3. Use data[indices] as initial centroids.\n",
    "\n",
    "# 81. Create a function to compute pairwise distances between points\n",
    "# Hint: Use broadcasting or torch.cdist for pairwise Euclidean distances.\n",
    "# Steps:\n",
    "# 1. Given two tensors X (N, D) and Y (M, D).\n",
    "# 2. Use torch.cdist(X, Y) or implement manually with broadcasting.\n",
    "\n",
    "# 82. Implement a simple convolution operation using tensor operations\n",
    "# Hint: Use unfold for im2col, then matmul with kernel, or use F.conv2d.\n",
    "# Steps:\n",
    "# 1. Given input tensor (N, C, H, W) and kernel (out_C, in_C, kH, kW).\n",
    "# 2. Use torch.nn.functional.unfold to extract patches.\n",
    "# 3. Multiply patches by kernel and sum.\n",
    "\n",
    "# 83. Create a function to generate random augmentation parameters\n",
    "# Hint: Use torch.rand or torch.randint for random crop, flip, rotation, etc.\n",
    "# Steps:\n",
    "# 1. Decide which augmentations (e.g., crop, flip, rotate).\n",
    "# 2. Generate random values for each parameter.\n",
    "# 3. Return as a dict or tuple.\n",
    "\n",
    "# 84. Implement attention mechanism computations using tensors\n",
    "# Hint: Use Q, K, V matrices and softmax(QK^T/sqrt(d_k))V.\n",
    "# Steps:\n",
    "# 1. Given Q, K, V tensors (batch, seq, d_k).\n",
    "# 2. Compute attention scores: Q @ K.transpose(-2, -1) / sqrt(d_k).\n",
    "# 3. Apply softmax to scores.\n",
    "# 4. Multiply by V to get output.\n",
    "\n",
    "# 85. Create a function to compute moving averages efficiently\n",
    "# Hint: Use torch.cumsum and slicing for moving average.\n",
    "# Steps:\n",
    "# 1. Given a 1D tensor and window size.\n",
    "# 2. Compute cumulative sum.\n",
    "# 3. Subtract shifted cumsum to get window sums.\n",
    "# 4. Divide by window size for average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c92b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 8: DEBUGGING & TROUBLESHOOTING (86-95)\n",
    "# ============================================\n",
    "\n",
    "# 86. Write a function to debug tensor shapes in a complex operation\n",
    "# Hint: Print shapes at key steps in your computation pipeline.\n",
    "# Steps:\n",
    "# 1. Define a function that takes tensors as input.\n",
    "# 2. Print tensor.shape at each step.\n",
    "\n",
    "# 87. Implement error handling for common tensor operation failures\n",
    "# Hint: Use try/except blocks to catch RuntimeError or ValueError.\n",
    "# Steps:\n",
    "# 1. Wrap tensor operations in try/except.\n",
    "# 2. Print or log the error message for debugging.\n",
    "\n",
    "# 88. Create a tensor validator that checks for NaN and infinite values\n",
    "# Hint: Use torch.isnan and torch.isinf to check for invalid values.\n",
    "# Steps:\n",
    "# 1. Define a function that takes a tensor.\n",
    "# 2. Use torch.isnan(tensor).any() and torch.isinf(tensor).any().\n",
    "# 3. Return or print a warning if found.\n",
    "\n",
    "# 89. Implement a function to visualize tensor statistics\n",
    "# Hint: Print or plot mean, std, min, max, and optionally histogram.\n",
    "# Steps:\n",
    "# 1. Compute tensor.mean(), tensor.std(), tensor.min(), tensor.max().\n",
    "# 2. Optionally use matplotlib to plot a histogram.\n",
    "\n",
    "# 90. Create a debugging tool that prints tensor info (shape, dtype, device, etc.)\n",
    "# Hint: Print tensor.shape, tensor.dtype, tensor.device, tensor.requires_grad.\n",
    "# Steps:\n",
    "# 1. Define a function that takes a tensor.\n",
    "# 2. Print all relevant info.\n",
    "\n",
    "# 91. Implement gradient checking for custom operations\n",
    "# Hint: Use torch.autograd.gradcheck for numerical gradient checking.\n",
    "# Steps:\n",
    "# 1. Define a custom autograd Function.\n",
    "# 2. Use gradcheck with double precision inputs.\n",
    "\n",
    "# 92. Create a function to detect memory leaks in tensor operations\n",
    "# Hint: Use Python's gc module and torch.cuda.memory_allocated().\n",
    "# Steps:\n",
    "# 1. Monitor memory usage before and after operations.\n",
    "# 2. Use gc.collect() to force garbage collection.\n",
    "\n",
    "# 93. Implement a tensor comparison function with tolerance\n",
    "# Hint: Use torch.allclose for elementwise comparison with tolerance.\n",
    "# Steps:\n",
    "# 1. Define a function that takes two tensors and atol/rtol.\n",
    "# 2. Return torch.allclose(tensor1, tensor2, atol, rtol).\n",
    "\n",
    "# 94. Create a profiling decorator for tensor operations\n",
    "# Hint: Use time.time() or torch.profiler to measure execution time.\n",
    "# Steps:\n",
    "# 1. Define a decorator that times a function.\n",
    "# 2. Print or log the elapsed time.\n",
    "\n",
    "# 95. Implement a function to optimize tensor layouts for performance\n",
    "# Hint: Use .contiguous(), .pin_memory(), or .to(memory_format=...).\n",
    "# Steps:\n",
    "# 1. Check if tensor is contiguous; if not, call .contiguous().\n",
    "# 2. For DataLoader, use .pin_memory() for faster host-to-GPU transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# BONUS CHALLENGES (96-100)\n",
    "# ========================================\n",
    "\n",
    "# 96. Implement a custom tensor class that extends PyTorch tensors\n",
    "\n",
    "# 97. Create a tensor caching mechanism for repeated operations\n",
    "\n",
    "# 98. Implement automatic mixed precision for tensor operations\n",
    "\n",
    "# 99. Create a distributed tensor operation using multiple devices\n",
    "\n",
    "# 100. Implement a complete mini neural network using only tensor operations\n",
    "\n",
    "# ========================================\n",
    "# CONGRATULATIONS!\n",
    "# ========================================\n",
    "# If you've completed all these exercises, you've mastered PyTorch tensors!\n",
    "# You're now ready to dive deep into neural network architectures,\n",
    "# custom layers, and advanced deep learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
